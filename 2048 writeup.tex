\documentclass{article}
\usepackage{graphicx} % Required for inserting images

% these two packages needed for argmax
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}

% packages needed for pseudocode
\usepackage{algorithm} 
\usepackage{algpseudocode} 

% Needed for subcaption for side-by-side figures
\usepackage{subcaption}

% package to indent first paragraph after section header
\usepackage{indentfirst}

% make tables and figures stay in correct section
\usepackage{float}

\usepackage[table]{xcolor}  % allow coloured cells

\title{2048 AI: Probabilistic Monte Carlo Tree Search}
\author{Noah Ripstein}
\date{June 2023}

\begin{document}

\maketitle

\section{Introduction}

2048 is an addictive game which was originally released in 2014.  The game begins with two randomly placed tiles, each having a value of either 2 or 4, randomly placed on a 4x4 grid. The player can move the tiles in four directions: up, down, left, or right. When a direction is chosen, all tiles on the grid slide as far as possible in that direction, merging with any adjacent tiles of the same value to form a new tile with double the value.  The value of the new tile is added to the score. After the player's turn,  a new tile spawns in a random location; this new tile has a 90\% chance of being a 2, and a 10\% chance of being a 4.   The game ends when the board is filled with tiles and the player has no legal moves.
TALK HERE ABOUT WHAT I DID BRIEFLY



\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{original_ss.jpeg}
    \caption{Original game}
    \label{fig:original_ss}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{ss_of_mine.png}
    \caption{My implementation}
    \label{fig:ss_of_mine}
  \end{subfigure}
  \caption{Screenshots from original game and my implementation}
  \label{fig:screenshots}
\end{figure}
%Discuss goal of game\\
%how its played briefly\\
%after the player selects a move, a new time spawns in a random empty location. It has a 90\% chance of being a 2, and a 10\% chance of being a 4.

\section{The Decision Tree}

One of the challenges of creating an AI which plays 2048 is the sheer number of possible games.  Figure 1 represents the possible board positions after the player makes only one move.  If there are 8 free spaces on the board, for example, then there are 64 possible game states after the player's move (assuming each of left, right, up and down are legal moves which do not combine any tiles).  In general, there are $2(l)(m)$ possible states after the player's move, where $l$ is the number of legal moves, and $m$ is the number of empty spaces on the board after tiles have been combined from the player's turn.

%\includegraphics[scale=0.2]{tree_1.jpeg}\\
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{tree_1.jpeg}
\caption{Decision tree representing possible board states after each move}
\label{fig:tree1}
\end{figure}

\section{Monte Carlo Tree Exploration: AI Designs}
\subsection{Pure Monte Carlo Tree Search}
The initial algorithm I employed is a Pure Monte Carlo Tree Search (Algorithm 1). This algorithm takes the current game position and the desired number of simulations per direction ($n$) as inputs. It explores all legal next moves from the current position by simulating $n$ games for each potential move. The scores from the end of these simulated games are then averaged to determine the desirability of each move. The direction with the highest average score is selected: 

\begin{equation}
  \textrm{Selected Move} = \argmax_{move} \text{MCTS1}(move)
\end{equation}


This approach initiates from the green nodes in the game tree diagram (Fig. 2). From there, the algorithm proceeds through random exploration to reach the yellow and red child nodes, representing the spawning of a 2 or 4 tile in each possible location.

% There are a few problems I identified with this approach.  Notice that it begins the search from a green node (Fig. 1), and reaches the yellow and red child nodes through random exploration. \\
While this approach provides a comprehensive exploration of the game tree, it has significant limitations. The primary concern lies in the random nature of the search process. As a consequence, some of the simulated games performed during the Monte Carlo simulations may yield exceptionally poor results that are highly unlikely to occur in actual gameplay.  This lead me to want to discard a portion of those simulated games with particularly poor scores from consideration.

Simply modifying Algorithm 1 to calculate the average score for a given move using only top-performing of simulated games would not adequately address this source of randomness, however.  There are two sources of randomness inherent in the Pure Monte Carlo Tree Search: randomness associated with game-play (which we aim to reduce), and randomness of tile spawns.  Discarding randomly played games with low scores in an attempt to address the former source of randomness might prevent the AI from evaluating branches of the tree which involve unlucky tiles spawning after the next turn.

% could also use delta performance. that way poor moves would be punished more I think 
\begin{algorithm}
    \caption{Pure Monte Carlo Tree Search}
    \begin{algorithmic}[1]
        \Function{MCTS1}{$\text{currentPosition, n}$}
            \State $\text{directionScores} \gets \text{list()}$
            \For{$\text{nextMove} \in \text{legalMoves}(\text{currentPosition})$}
                \State $\text{nextMoveScores} \gets \text{list()}$
                \For{$\text{gameNumber} \gets 1 \text{ to } n$}
                    \State $\text{result} \gets \text{playRandomGame}(\text{currentPosition}, \text{nextMove})$
                    \State $\text{nextMoveScores}.\text{append}(\text{result})$
                \EndFor
                \State $\text{averageScore} \gets \text{mean(nextMoveScores)}$
                \State $\text{directionScores}.\text{append}(\text{averageScore})$
            \EndFor
            % \State $\text{bestMove} \gets \text{legalMoves}(\text{currentPosition})[\text{scores}.\text{argmax}()]$
            \State \textbf{return} $\text{directionScores}$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Markov Decision Process}
The pitfalls of the Pure Monte Carlo Tree Search raised in section 3.1 can be circumvented by formalizing the game structure as a Markov Decision Process (MDP). Where the PMCTS algorithm begins Monte Carlo simulations from the board state after a player's move (green nodes in Figure 2), The MDP begins Monte Carlo simulations from each possible tile spawn in response to a player's move (red nodes in Figure 2).  It is possible to model the game as a markov decision process because the probability of reaching a future state depends only on the current state, not on previous states: 
$$P(s|s_{n-1}, s_{n-2},...,s_{0}) = P(s|s_{n-1}) $$

In general, an is MDP is characterized as follows:
\begin{enumerate}
	\item $S$: The set of states (board position and score) which could arise after the AI's next move.
	\item $A$: The set of actions which the AI could legally take from the current position.
	\item $P(s^\prime | s, a)$: Transition probability of reaching state $s^\prime$ given current state $s \in S$ after taking action $a \in A$.
	\item $V(s)$: The value function which determines the expected future reward associated with entering a state $s \in S$
	\item $\pi(s)$: The policy function which uses the other functions to strategically pick an action $a \in A$ given any board state.
	\item $R(s)$: The immediate reward associated with taking action $a \in A$ which leads to state $s \in S$ is a part of many MDP designs.  Justification for why this function was excluded can be found in section 3.3.
\end{enumerate}

$A$ can be constructed by checking which of left, right, up or down are legal moves. $S$ can be constructed by placing a 2 and then a 4 on each empty tile for each $a \in A$ (Algorithm \ref{CREATE_S}). The value function, $V(s, \vec{\theta})$,  (Algorithm \ref{value_function}) performs a Monte Carlo tree search, with the number of simulations determined by the parameter vector $\vec{\theta}$.  $\vec{\theta}$ contains the following parameters:
\begin{enumerate}
	\item Number of random searches for 2 spawning with 1-3 empty tiles.
	\item Number of random searches for 2 spawning with 4-6 empty tiles.
	\item Number of random searches for 2 spawning with 7-9 empty tiles.
	\item Number of random searches for 2 spawning with 10-15 empty tiles.
	\item How many times more searches 2 spawns should get compared to 4 spawns.
	\item Top proportion of best performing moves to return.
\end{enumerate}

$\theta_1, \theta_2, \theta_3$ are scaled so they have the same total number of simulations, and $\theta_4$ uses the same value for each of 10-15.  $\vec{\theta}$ remains constant throughout a single play-through of the game.  As will be discussed in SECTION NUMBER, optimizing $\vec{\theta}$ became a primary direction of inquiry. The policy function which determines what action $a \in A$ to take, $\pi(s)$, is given in Equation The policy function is given in Equation (\ref{MC12_policy_fn}):

\begin{equation}
\label{MC12_policy_fn}
\pi(s) = \argmax\limits_{a}\left( \sum\limits_{s' \in S} P(s'|s, a)V(s', \vec{\theta}) \right)
\end{equation}


Notice that $\pi(s)$ picks the action with the highest expected value, where the value associated with reaching a state is given by a Monte Carlo tree search.

%This is appropriate for $\theta_4$ because beyond early stages of the game, it is uncommon for there to be 10 or more empty tiles, which makes scaling this parameter unnecessarily computationally expensive.

Compared to the Pure Monte Carlo Tree Search, the MDP facilitates more sophisticated inference in two ways. 

\begin{enumerate}
  \item By discarding a portion of explored games with poor results at each node, the impact of the Monte Carlo search playing out extremely poor moves due to chance can be mitigated.  Crucially, simulations with lower scores due to "unlucky" tile spawns in the subsequent turn are not eliminated, ensuring a more comprehensive exploration of potential game outcomes.
  \item Unlike the Pure Monte Carlo Tree Search, where game states in which a 2 spawns after the players turn are explored 9x as frequently as those in which a 4 spawns, the number of Monte Carlo searches on each node type can be made independent.  This can guarantee that all nodes are explored at least 3 times, which gives far more information than a node being explored 1 time, but does not significantly increase runtime.  
 
\end{enumerate}

The MDP is implemented in a manner which makes customizable the number of Monte Carlo searches per node ($\theta_1 - \theta_5$) and proportion of top-performing simulated scores to keep ($\theta_6$).  Each of these parameters impact the reported score associated with a direction, and therefore the move selected by $\pi(s)$.  


\begin{algorithm}
    \caption{Generation of $S$}
    \label{CREATE_S}
    \begin{algorithmic}[1]
    		\Function{getStates}{A}
    		\State $S = \O$
            \For{$a \in A$}
                \State $c_s \gets$ score associated with taking action $a$
                \For{tileNum $\in \{2, 4 \}$}
                	\For{tile in emptyTiles}
                		\State place tileNum on tile
                		\State Add $s \gets$ (board, $c_s$) to $S$
                		\State remove tileNum from tile
                	\EndFor
                \EndFor
            \EndFor
		\State \textbf{Return }$S$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Value Function: $V(s, \vec{\theta})$}
    \label{value_function}
    \begin{algorithmic}[1]
        \Function{Value}{$s, \vec{\theta}$}
            \State $resultList \gets$ empty list
            \For{$i \gets 1$ to $\theta_{\text{num\_sims}}$}\Comment{$\theta_{\text{num\_sims}}$ depends on empty tiles}
                \State $result \gets$ \Call{RandomGame}{$s$} \Comment{Call the RandomGame function}
                \State add $result$ to $resultList$ \Comment{Record the result}
            \EndFor
            \State sort $resultList$ in ascending order
            \State $proportion \gets \theta_{\text{proportion}} \times \text{length}(resultList)$
            \State $topResults \gets resultList[1:\text{round}(proportion)]$ \Comment{Select the top results}
            \State \textbf{return} $topResults$
        \EndFunction
        \Statex
        \Function{RandomGame}{$s$}
            \While{game not over}
                \State Pick random $a \in A$
                \State Make move $a$
            \EndWhile
            \State \textbf{return} game score
        \EndFunction
    \end{algorithmic}
\end{algorithm}

%\section{Next Move Selection}
%The aim is to maximize the expected score.  Expected value calculations are helpful for this.  Let $move_j$ be the $j$th legal move in the current game state, $\overline{S_{n_i}}$ be the average score obtained in the $i$th node of the Monte Carlo search with tile value $n$ as parent, and $m$ be the number of empty tiles (given by the cardinality of $\overline{S_{n_i}}$). % THIS NEEDS REWORDING\\
%TALK ABOUT HOW THE AVERAGE THING ISNT A GREAT ESTIMATE OF PAYOUT. THEN CAN ALSO MENTION HOW THAT WOULD BE GOOD FOR FUTURE DIRECTIONS
%$$E[move_j] = \sum_{i=1}^m \left(\frac{9}{10}\frac{1}{m}\overline{S_{2_i}} + \frac{1}{10}\frac{1}{m}\overline{S_{4_i}}\right)$$
%$$E[move_j] = \frac{1}{m}\sum_{i=1}^m \left(\frac{9}{10}\overline{S_{2_i}} + \frac{1}{10}\overline{S_{4_i}}\right)$$
%$$\textrm{Selected Move} = \argmax_{move} E[move]$$

\section{Results}
Have chart of \% reached 2048, 4096, etc, and avg score for each model. probs need each of them at 100 runs.

can have some sort of Bayesian BDF bc can model the proportion of time it reaches 2048 as berneulli so can do classic PDF.\\


\begin{table}[h]
  \caption{Results of Different Models}
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    & 2048 & 4096 & 8192 & Avg. Score \\
    \hline
    Pure MCTS & & & & \\
    \hline
    MDP (top 100\%) & \cellcolor{green!25}97\% & \cellcolor{green!25}58\% & 0\% & 53,232\\
    \hline
    MDP (top 75\%) & 94\% & \cellcolor{green!25}58\% & 1\% & 53,565\\
    \hline
    MDP (top 50\%) & 94\% & 57\% & 2\% & 53,282\\
    \hline
    MDP (top 25\%) & 93\% & \cellcolor{green!25}58\% & \cellcolor{green!25}5\% & \cellcolor{green!25}55,966 \\
    \hline
  \end{tabular}
\end{table}

\section{Future Directions (not done)}

MAIN IDEAS FOR FUTURE DIRECTIONS:\\
- optimize $\vec{\theta}$ using surrogate modelling:
\begin{enumerate}
	\item Evaluate model with a series of parameters created by latin hypercube sampling
	\item Design cost function which should be minimized to have "optimal performance". Will need to trade off speed and performance. probably ideally will have on average 2 seconds per move and reward higher scores.
	\item EITHER Train some sort of machine learning model to approximate the underlying function, and then minimize that function's cost
	\item OR use Bayesian optimization so I can get a better sense of uncertainty.
\end{enumerate}



Use heuristics\\
Note that part of the challenge here was not to use heuristics
https://arxiv.org/abs/2110.10374
These stanford math profs made a deep reinforcement learning model that doesnt use heuristics and mine is better than it I think (theirs seems to be for a class they taught rather than research really). Check performance to be sure.

dynamically switch to minimax or expectimax when there are few open tiles.\\




\section{FORMALIZING AS MDP (notes to self)}
The components of a Markov Decision process are:
%https://leonardoaraujosantos.gitbook.io/artificial-inteligence/artificial_intelligence/markov_decision_process

\begin{enumerate}
	\item States: A set of possible states (board positions)
	\item Tansition model: $P(s'|s, a)$ The probability of reaching state $s'$ given that we perform action $a$ while in state $s$.
	\item Action: $A(s)$ set of possible actions in a state (what's legal of up, down, left right)
	
	\item Value function: $V(s)$ normally recursively defined with bellman equations,  gives the value associated with a given state.
	\item Policy: function takes in a state, returns an action $a$. can be given by 
	
	
	$\pi(s) = \argmax\limits_{a}\left( \sum\limits_{s' \in S} P(s'|s, a)(R(s,a) + \gamma V(s')) \right)$ \\
	MC12 uses:
	$\pi(s) = \argmax\limits_{a}\left( \sum\limits_{s' \in S} P(s'|s, a)V(s') \right)$\\
	$R(s, a)$ is the immediate score we get if we perform action $a$ while in state $s$.
	$\gamma$ is discount rate for future rewards (follows principle that certain reward now better than uncertain reward later)
	$V(s')$ is the value of reaching state $s'$. Done with monte carlo tree search.
\end{enumerate}




\end{document}


